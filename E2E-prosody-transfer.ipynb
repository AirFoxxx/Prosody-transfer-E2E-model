{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End prosody transfer prototype using QuartzNet, Mellotron, Tacotron, WaveGlow, Multi-Singer and HiFiGAN.\n",
        "This notebook uses resources from following repositories: \n",
        " - https://github.com/NVIDIA/mellotron\n",
        " - https://github.com/NVIDIA/NeMo\n",
        " - https://github.com/Rongjiehuang/Multi-Singer\n",
        " - https://github.com/NVIDIA/waveglow.git\n",
        "\n",
        "and is designed to run in google colab. If you want to run it locally, you might be required to install many additional dependencies!"
      ],
      "metadata": {
        "id": "7UqbF9QmMbwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook requires a GPU to run properly. \n",
        "First, you must select in the top left corner => `Runtime` / `Change runtime type` => and here select a `GPU` option."
      ],
      "metadata": {
        "id": "YaoT6NglQXdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting a personal google drive with pretrained models: choose the correct account and allow access..."
      ],
      "metadata": {
        "id": "30wFaUQLSlQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting a personal google drive with pretrained models: choose the correct account and allow access...\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hMMbP_NCSaZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "! ls"
      ],
      "metadata": {
        "id": "xDblKoRTU8IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*If the repo already exists in your local files, run the cell below to get rid of it and instatantiate new copy*\n"
      ],
      "metadata": {
        "id": "ZMOQ9Z2VOA_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove repository from previous testing instances\n",
        "! rm -r mellotron\n",
        "# clone the Mellotron repository\n",
        "! git clone https://github.com/NVIDIA/mellotron.git"
      ],
      "metadata": {
        "id": "ixzSE8cddXm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descend into the repo and list directory contents"
      ],
      "metadata": {
        "id": "PfPenxYnOZ83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/mellotron\n",
        "! ls"
      ],
      "metadata": {
        "id": "8FMH_hs0OWCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Nvidia NeMo Framework + dependencies\n",
        "The following block will import the NeMo framework from github."
      ],
      "metadata": {
        "id": "_EFrTpVlYRAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install unidecode\n",
        "\n",
        "# ## Install NeMo\n",
        "BRANCH = 'main'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
        "\n",
        "## Install TorchAudio\n",
        "!pip install torchaudio>=0.10.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "## Grab the config we'll use in this example\n",
        "!mkdir configs"
      ],
      "metadata": {
        "id": "zgvrDMTkYXh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nemo\n",
        "nemo.__version__"
      ],
      "metadata": {
        "id": "kS6u9RJ9Yk50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing Git submodule for waveglow\n",
        "WaveGlow is used as a default vocoder for Mellotron."
      ],
      "metadata": {
        "id": "v4oNsWTIOfGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell makes sure the sub-repository initially used by Mellotron is intialized\n",
        "! git submodule init\n",
        "! git submodule update"
      ],
      "metadata": {
        "id": "TVqFN8b3OjnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy large files (pretrained models) from the mapped GDrive to the local filesystem."
      ],
      "metadata": {
        "id": "JR1rWm-0O7kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/models/ .\n",
        "! ls"
      ],
      "metadata": {
        "id": "2OwKySjnO5P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mellotron needs many dependencies as described by requirements.txt\n",
        "- matplotlib==2.1.0\n",
        "- tensorflow==1.15.2\n",
        "- inflect==0.2.5\n",
        "- librosa==0.6.0\n",
        "- scipy==1.0.0\n",
        "- tensorboardX==1.1\n",
        "- Unidecode==1.0.22\n",
        "- pillow\n",
        "- nltk==3.4.5\n",
        "- jamo==0.4.1\n",
        "- music21"
      ],
      "metadata": {
        "id": "rDu53pqsPMIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create a new requirement file, since google colab comes with many of these programs preinstalled and their succesive installation throws errors.\n",
        "! touch newReqs.txt\n",
        "! echo 'Unidecode==1.0.22' > newReqs.txt\n",
        "! echo 'tensorflow==1.15.2' >> newReqs.txt"
      ],
      "metadata": {
        "id": "i5JtnfHyPJM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Unidecode is not preinstalled by colab.*\n",
        "*Tensorflow is included in colab, but versions 0.2.0 and newer removed an attribute called \"contrib\". Mellotron requires a older version of tensorflow to function properly.*"
      ],
      "metadata": {
        "id": "0Havng2aP7U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# requirement installation - can take a while.\n",
        "! pip install -r newReqs.txt"
      ],
      "metadata": {
        "id": "HV7WRspHQRrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mellotron Inference\n",
        "Mellotron needs to instantiate and import many of its libraries to synthesize mel spectrograms."
      ],
      "metadata": {
        "id": "BjC2M4IxQkPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "import sys\n",
        "sys.path.append('waveglow/')\n",
        "\n",
        "from itertools import cycle\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from scipy.io.wavfile import write\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "\n",
        "# importing custom Mellotron classes\n",
        "from hparams import create_hparams\n",
        "from model import Tacotron2, load_model\n",
        "from waveglow.denoiser import Denoiser\n",
        "from layers import TacotronSTFT\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from text import cmudict, text_to_sequence\n",
        "from mellotron_utils import get_data_from_musicxml"
      ],
      "metadata": {
        "id": "h0HVOgATQ3Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def panner(signal, angle):\n",
        "    angle = np.radians(angle)\n",
        "    left = np.sqrt(2)/2.0 * (np.cos(angle) - np.sin(angle)) * signal\n",
        "    right = np.sqrt(2)/2.0 * (np.cos(angle) + np.sin(angle)) * signal\n",
        "    return np.dstack((left, right))[0]"
      ],
      "metadata": {
        "id": "A7913fOdQ99t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mel_f0_alignment(mel_source, mel_outputs_postnet, f0s, alignments, figsize=(16, 16)):\n",
        "    fig, axes = plt.subplots(4, 1, figsize=figsize)\n",
        "    axes = axes.flatten()\n",
        "    axes[0].imshow(mel_source, aspect='auto', origin='bottom', interpolation='none')\n",
        "    axes[1].imshow(mel_outputs_postnet, aspect='auto', origin='bottom', interpolation='none')\n",
        "    axes[2].scatter(range(len(f0s)), f0s, alpha=0.5, color='red', marker='.', s=1)\n",
        "    axes[2].set_xlim(0, len(f0s))\n",
        "    axes[3].imshow(alignments, aspect='auto', origin='bottom', interpolation='none')\n",
        "    axes[0].set_title(\"Source Mel\")\n",
        "    axes[1].set_title(\"Predicted Mel\")\n",
        "    axes[2].set_title(\"Source pitch contour\")\n",
        "    axes[3].set_title(\"Source rhythm\")\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "BSXghgqoQ_sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mel(path):\n",
        "    audio, sampling_rate = librosa.core.load(path, sr=hparams.sampling_rate)\n",
        "    audio = torch.from_numpy(audio)\n",
        "    if sampling_rate != hparams.sampling_rate:\n",
        "        raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
        "            sampling_rate, stft.sampling_rate))\n",
        "    audio_norm = audio.unsqueeze(0)\n",
        "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "    melspec = stft.mel_spectrogram(audio_norm)\n",
        "    melspec = melspec.cuda()\n",
        "    return melspec"
      ],
      "metadata": {
        "id": "r_3k6rURREy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = create_hparams()"
      ],
      "metadata": {
        "id": "ai2jFdT-RKVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Settings for the inner Tacotron model used as aligner."
      ],
      "metadata": {
        "id": "_J0_UmUvRPxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "                    hparams.mel_fmax)"
      ],
      "metadata": {
        "id": "1yR4To5xRNgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading pretrained models\n",
        "The pretrained libriTts model for Mellotron is available from: https://drive.google.com/open?id=1ZesPPyRRKloltRIuRnGZ2LIUEuMSVjkI"
      ],
      "metadata": {
        "id": "kgmIGG0YR7qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"models/mellotron_libritts.pt\"\n",
        "mellotron = load_model(hparams).cuda().eval()\n",
        "mellotron.load_state_dict(torch.load(checkpoint_path)['state_dict'])"
      ],
      "metadata": {
        "id": "vzk-mwV2SBgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pretrained model for WaveGlow is available from: https://drive.google.com/open?id=1okuUstGoBe_qZ4qUEF8CcwEugHP7GM_b"
      ],
      "metadata": {
        "id": "1dlnNBYlSeJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "waveglow_path = 'models/waveglow_256channels_universal_v4.pt'\n",
        "waveglow = torch.load(waveglow_path)['model'].cuda().eval()\n",
        "denoiser = Denoiser(waveglow).cuda().eval()"
      ],
      "metadata": {
        "id": "UW9HZ-jOSKBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add full ASR NeMo Pipeline with Quartznet\n",
        "To demonstrate teh power of NeMo, we can add a Speech recognition model that will transcribe the text from source audio clip for us."
      ],
      "metadata": {
        "id": "SFpZkgO1aUlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nemo.collections.asr as nemo_asr"
      ],
      "metadata": {
        "id": "msX03rfmakQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pretrained quartznet\n",
        "quartznet = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=\"QuartzNet15x5Base-En\")"
      ],
      "metadata": {
        "id": "F6C_nYumamhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = ['./data/test-sing.wav']\n",
        "for fname, transcription in zip(files, quartznet.transcribe(paths2audio_files=files)):\n",
        "  print(f\"Audio in {fname} was recognized as: {transcription}\")"
      ],
      "metadata": {
        "id": "HS3dbMPJaoEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load additional data required\n",
        "Arpabet is a giant text file that maps normalized text into phonemes with correct pronunciation. For example \"ABDOMINAL ==> AE0 B D AA1 M AH0 N AH0 L\" \n",
        "\n",
        "audio_paths contains audio/text pairs for inference."
      ],
      "metadata": {
        "id": "KSI_yCDOSpCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arpabet_dict = cmudict.CMUDict('data/cmu_dictionary')\n",
        "audio_paths = 'data/examples_filelist.txt'\n",
        "dataloader = TextMelLoader(audio_paths, hparams)\n",
        "datacollate = TextMelCollate(1)"
      ],
      "metadata": {
        "id": "FCotSlZDTCgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loading is done here:"
      ],
      "metadata": {
        "id": "M6QuMWVNTJVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_idx = 0\n",
        "audio_path, text, sid = dataloader.audiopaths_and_text[file_idx]\n",
        "\n",
        "# get audio path, encoded text, pitch contour and mel for gst\n",
        "text_encoded = torch.LongTensor(text_to_sequence(text, hparams.text_cleaners, arpabet_dict))[None, :].cuda()    \n",
        "pitch_contour = dataloader[file_idx][3][None].cuda()\n",
        "mel = load_mel(audio_path)\n",
        "print(audio_path, text)\n",
        "\n",
        "# load source data to obtain rhythm using tacotron 2 as a forced aligner\n",
        "x, y = mellotron.parse_batch(datacollate([dataloader[file_idx]]))"
      ],
      "metadata": {
        "id": "tf-ZYMDCSvy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the selected source audioclip for Prosody extraction."
      ],
      "metadata": {
        "id": "U0IxOi7OTZJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(audio_path, rate=hparams.sampling_rate)"
      ],
      "metadata": {
        "id": "gdVfauCcTPyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tbCmF6oHTc0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract speaker IDs from dataset\n",
        "speaker_ids = TextMelLoader(\"filelists/libritts_train_clean_100_audiopath_text_sid_shorterthan10s_atleast5min_train_filelist.txt\", hparams).speaker_ids\n",
        "# Extract speaker information\n",
        "speakers = pd.read_csv('filelists/libritts_speakerinfo.txt', engine='python',header=None, comment=';', sep=' *\\| *', \n",
        "                       names=['ID', 'SEX', 'SUBSET', 'MINUTES', 'NAME'])\n",
        "\n",
        "# Connect speaker information with ID\n",
        "speakers['MELLOTRON_ID'] = speakers['ID'].apply(lambda x: speaker_ids[x] if x in speaker_ids else -1)\n",
        "# Create speaker list based on SEX and length of recordings\n",
        "female_speakers = cycle(\n",
        "    speakers.query(\"SEX == 'F' and MINUTES > 20 and MELLOTRON_ID >= 0\")['MELLOTRON_ID'].sample(frac=1).tolist())\n",
        "male_speakers = cycle(\n",
        "    speakers.query(\"SEX == 'M' and MINUTES > 20 and MELLOTRON_ID >= 0\")['MELLOTRON_ID'].sample(frac=1).tolist())"
      ],
      "metadata": {
        "id": "ijZr0h5NTbb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prosody transfer"
      ],
      "metadata": {
        "id": "_BtxXHbgUOYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    # get rhythm (alignment map) using tacotron 2\n",
        "    mel_outputs, mel_outputs_postnet, gate_outputs, rhythm = mellotron.forward(x)\n",
        "    rhythm = rhythm.permute(1, 0, 2)"
      ],
      "metadata": {
        "id": "p1mrfI2eULoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose random speaker ID and SEX for synthesis\n",
        "speaker_id = next(female_speakers) if np.random.randint(2) else next(male_speakers)\n",
        "speaker_id = torch.LongTensor([speaker_id]).cuda()\n",
        "\n",
        "# Generate spectrogram\n",
        "with torch.no_grad():\n",
        "    mel_outputs, mel_outputs_postnet, gate_outputs, _ = mellotron.inference_noattention(\n",
        "        (text_encoded, mel, speaker_id, pitch_contour, rhythm))\n",
        "\n",
        "#Plot spectrogram and addditional info\n",
        "plot_mel_f0_alignment(x[2].data.cpu().numpy()[0],\n",
        "                      mel_outputs_postnet.data.cpu().numpy()[0],\n",
        "                      pitch_contour.data.cpu().numpy()[0, 0],\n",
        "                      rhythm.data.cpu().numpy()[:, 0].T)"
      ],
      "metadata": {
        "id": "H-1zBvRRUUW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see the original spectrogram with the predicted spectrogram that will be synthesized into audio. \n",
        "The third graph displays predicted pitch contour for the clip.\n",
        "The last graph shows the alignment of spectrograms."
      ],
      "metadata": {
        "id": "Ur5VcYGcUk43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Waveform generation with WaveGlow"
      ],
      "metadata": {
        "id": "Wi23naFjU531"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    audioWaveglow = denoiser(waveglow.infer(mel_outputs_postnet, sigma=0.8), 0.01)[:, 0]\n",
        "ipd.Audio(audioWaveglow[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "metadata": {
        "id": "u7Qib_kbU3Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replacing WaveGlow with Multi-Singer\n",
        "This is an attempt to replace waveglof for audiowave generation with a better-suited vocoder - Multisinger."
      ],
      "metadata": {
        "id": "P_8jyjNbV0gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# return to the absolute path root\n",
        "%cd /content/\n",
        "! git clone https://github.com/Rongjiehuang/Multi-Singer.git\n",
        "%cd Multi-Singer"
      ],
      "metadata": {
        "id": "VHfB6QcaWIvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infernece for Multi-Singer\n",
        "The inference for multi-singer is different as it is done from command line via a comand with parameters.\n",
        "\n",
        "\n",
        "`python inference.py -i data/feature -o outputs/  -c checkpoints/*.pkl -g config/config.yaml`\n",
        "\n",
        "-i acoustic feature folder\n",
        "\n",
        "-o directory to save generated speech\n",
        "\n",
        "-c checkpoints file\n",
        "\n",
        "-c config file"
      ],
      "metadata": {
        "id": "AK2rwYgzWSwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make directory for output\n",
        "! mkdir outputs"
      ],
      "metadata": {
        "id": "AB4YnfrRWiRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create and save mel spectrogram representation from Mellotron\n",
        "file = open(\"mel_output.pt\", \"w\")\n",
        "\n",
        "# it is difficult to determine, in what format is the spectrogram and how to properly save it for import into Multi-singer\n",
        "torch.save(mel_outputs_postnet, 'mel_output.pt')"
      ],
      "metadata": {
        "id": "GWQg8nOzWuQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating audio waveform with Multi-singer\n",
        "So far this always fails for not accepting the input spectrogram format."
      ],
      "metadata": {
        "id": "2XI73pzVXu66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python inference.py -i mel_output.pt -o outputs/  -c ../models/Basic.pkl -g config/config.yaml"
      ],
      "metadata": {
        "id": "eXKsRMjHXtQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating audio waveform with HiFiGAN via NeMo\n",
        "The following approach demonstrates the possibility of swapping a vocoder module for Mellotron and attaching a different module provided by the NeMo framework."
      ],
      "metadata": {
        "id": "suZAIHgnYAom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pretrained TTS models\n",
        "import nemo.collections.tts as nemo_tts\n",
        "\n",
        "# import pretrained HiFiGAN\n",
        "from nemo.collections.tts.models import HifiGanModel\n",
        "hifigan = HifiGanModel.from_pretrained(\"tts_hifigan\").eval().cuda()"
      ],
      "metadata": {
        "id": "B-LCeSmAZBWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Audio synthesis from original mel-Spectrogram\n",
        "The following audio was synthesized via HiFiGAN."
      ],
      "metadata": {
        "id": "P_f1SISiZNnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio = hifigan.convert_spectrogram_to_audio(spec=mel_outputs_postnet).to('cpu').detach().numpy()\n",
        "ipd.display(ipd.Audio(audio, rate=hparams.sampling_rate))"
      ],
      "metadata": {
        "id": "AEgbhZQPZLGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOr comparison, the previous WaveGlow synthesized clip..."
      ],
      "metadata": {
        "id": "cyFZtDzsZoi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ipd.Audio(audioWaveglow[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "metadata": {
        "id": "NNCqjJ2mZles"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "E2E-prosody-transfer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}